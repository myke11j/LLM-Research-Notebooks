{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Install Libraries"
      ],
      "metadata": {
        "id": "j2-xSlAKWumj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ini53LxkV_Yq"
      },
      "outputs": [],
      "source": [
        "!pip install torch transformers accelerate bitsandbytes -U\n",
        "!pip install sentencepiece\n",
        "!pip install gradio"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import libraries"
      ],
      "metadata": {
        "id": "GO1qSH-SWyTD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import re\n",
        "from google.colab import userdata\n",
        "import gradio as gr\n",
        "HUGGING_FACE_TOKEN = userdata.get(\"HUGGING_FACE_TOKEN\")"
      ],
      "metadata": {
        "id": "ONXNbqVmWF2d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\""
      ],
      "metadata": {
        "id": "ZPwMzeXCWJUy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Model"
      ],
      "metadata": {
        "id": "hs1q5nfjW244"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")"
      ],
      "metadata": {
        "id": "yb3N_ZA1WOXU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\", token=HUGGING_FACE_TOKEN,)\n",
        "model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\", quantization_config=bnb_config, token=HUGGING_FACE_TOKEN,)"
      ],
      "metadata": {
        "id": "b2i6rLQZWPxI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup function whill keep track of chat template and inversion"
      ],
      "metadata": {
        "id": "V4h_9ZcrW6OY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "messages = []"
      ],
      "metadata": {
        "id": "S8sCaeYrWVXU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def send_promptTo_MistralAI(question):\n",
        "  global messages\n",
        "  messages.append({'role': 'user', 'content': question})\n",
        "  encodeds = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n",
        "\n",
        "  model_inputs = encodeds.to(device)\n",
        "  model.to(device)\n",
        "\n",
        "  generated_ids = model.generate(model_inputs, max_new_tokens=1000, do_sample=True)\n",
        "  decoded = tokenizer.batch_decode(generated_ids)\n",
        "  response = decoded[0]\n",
        "  matches = list(re.finditer(r'\\[/INST\\]', response))\n",
        "\n",
        "  if matches:\n",
        "      last_match = matches[-1]\n",
        "      start = last_match.end()\n",
        "      response = response[start:].strip()\n",
        "  response = response.replace('</s>', '')\n",
        "  messages.append({'role': 'assistant', 'content': response})\n",
        "  return response\n"
      ],
      "metadata": {
        "id": "kSJO7gucWXiL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def slow_echo(message, history):\n",
        "    response = call_predict_api(message)\n",
        "    for i in range(len(response)):\n",
        "        time.sleep(0.05)\n",
        "        yield f\"{response[:i + 1]}\"\n",
        "\n",
        "def call_predict_api(message):\n",
        "    try:\n",
        "        response = send_promptTo_MistralAI(message)\n",
        "        return response\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        return f\"Error getting reply: {e}\"\n",
        "\n",
        "demo = gr.ChatInterface(slow_echo)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    demo.launch()"
      ],
      "metadata": {
        "id": "hl-Z1JFCWmRl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}