{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Install packages"
      ],
      "metadata": {
        "id": "vfD6kILDd2Fg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rfzFGcFgdtur"
      },
      "outputs": [],
      "source": [
        "!pip install torch\n",
        "!pip install -U transformers\n",
        "!pip install pyngrok"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import torch\n",
        "from flask import Flask, request, jsonify\n",
        "from pyngrok import ngrok\n",
        "from google.colab import userdata"
      ],
      "metadata": {
        "id": "nGxRTSOxeAXZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add Env variables and authenticate ngrok"
      ],
      "metadata": {
        "id": "lGg7Wp_BeM79"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NGROK_TOKEN = userdata.get(\"NGROK_TOKEN\")\n",
        "HUGGING_FACE_TOKEN = userdata.get(\"HUGGING_FACE_TOKEN\")"
      ],
      "metadata": {
        "id": "_D1UpoLueFu8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ngrok.set_auth_token(NGROK_TOKEN)"
      ],
      "metadata": {
        "id": "xhi5lk3keIJF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = \"meta-llama/Llama-3.2-3B-Instruct\""
      ],
      "metadata": {
        "id": "R1Q5yptgeT0U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model,token=HUGGING_FACE_TOKEN)"
      ],
      "metadata": {
        "id": "3UOltBqzeUQi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"meta-llama/Llama-3.2-3B-Instruct\",\n",
        "    token=HUGGING_FACE_TOKEN\n",
        ")"
      ],
      "metadata": {
        "id": "XhRoiiSiedRk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Deploy with Flask"
      ],
      "metadata": {
        "id": "5y9VB9Beeo4A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "app = Flask(__name__)"
      ],
      "metadata": {
        "id": "hXy_fX9den9n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@app.route('/predict', methods=['POST'])\n",
        "def predict():\n",
        "    data = request.json\n",
        "    prompt = data.get(\"prompt\", \"\")\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"Give honest answer for questions asked\"},\n",
        "        {\"role\": \"user\", \"content\": prompt},\n",
        "    ]\n",
        "\n",
        "    prompt = \"\\n\".join([f\"{message['role']}: {message['content']}\" for message in messages])\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    outputs = model.generate(\n",
        "        inputs[\"input_ids\"],  # Input tokens\n",
        "        max_new_tokens=256,   # Maximum number of tokens to generate\n",
        "        num_return_sequences=1,\n",
        "        do_sample=True,       # Enables sampling for more creative responses\n",
        "        top_k=50,             # Controls randomness\n",
        "        top_p=0.95,           # Nucleus sampling for diversity\n",
        "        temperature=0.2       # Creativity of the response\n",
        "    )\n",
        "\n",
        "    # Decode the output and return the response\n",
        "    response_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    print(response_text)\n",
        "    ai_bot_dialogue = re.findall(r'system: (.*?)(?=\\nuser:|$)', response_text, re.DOTALL)\n",
        "    if ai_bot_dialogue:\n",
        "      ai_bot_dialogue = ai_bot_dialogue[-1].strip()\n",
        "    return jsonify({\"response\": ai_bot_dialogue})"
      ],
      "metadata": {
        "id": "OLHkM0CXetjb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "port=5000\n",
        "\n",
        "public_url = ngrok.connect(port)\n",
        "print(f\" * Ngrok tunnel \\\"{public_url}\\\" -> \\\"http://127.0.0.1:5000\\\"\")\n",
        "app.run(host='0.0.0.0', port=port)"
      ],
      "metadata": {
        "id": "BTrLC5xfevhl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}